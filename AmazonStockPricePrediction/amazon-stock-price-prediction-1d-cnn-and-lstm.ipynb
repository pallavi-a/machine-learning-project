{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport seaborn as sns\nimport datetime as dt\nfrom datetime import datetime   \nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nsns.set_style(\"whitegrid\")\nfrom pandas.plotting import autocorrelation_plot\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use(\"ggplot\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv(\"../input/amazon-stock-price-all-time/Amazon.csv\")\ndata.info()\ndata.shape\n# data.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.reset_index(drop=True, inplace=True)\ndata.fillna(data.mean(), inplace=True)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.plot(legend=True,subplots=True, figsize = (12, 6))\nplt.show()\n\ndata.shape\ndata.size\ndata.describe(include='all').T\ndata.dtypes\ndata.nunique()\nma_day = [10,50,100]\n\nfor ma in ma_day:\n    column_name = \"MA for %s days\" %(str(ma))\n    data[column_name]=pd.DataFrame.rolling(data['Close'],ma).mean()\n\ndata['Daily Return'] = data['Close'].pct_change()\n# plot the daily return percentage\ndata['Daily Return'].plot(figsize=(12,5),legend=True,linestyle=':',marker='o')\nplt.show()\n\nsns.displot(data['Daily Return'].dropna(),bins=100,color='green')\nplt.show()\n\ndate=pd.DataFrame(data['Date'])\nclosing_df1 = pd.DataFrame(data['Close'])\nclose1  = closing_df1.rename(columns={\"Close\": \"data_close\"})\nclose2=pd.concat([date,close1],axis=1)\nclose2.head()\n\ndata.reset_index(drop=True, inplace=True)\ndata.fillna(data.mean(), inplace=True)\ndata.head()\n\ndata.nunique()\n\ndata.sort_index(axis=1,ascending=True)\n\ncols_plot = ['Open', 'High', 'Low','Close','Volume','MA for 10 days','MA for 50 days','MA for 100 days','Daily Return']\naxes = data[cols_plot].plot(marker='.', alpha=0.7, linestyle='None', figsize=(11, 9), subplots=True)\nfor ax in axes:\n    ax.set_ylabel('Daily trade')\n\nplt.plot(data['Close'], label=\"Close price\")\nplt.xlabel(\"Timestamp\")\nplt.ylabel(\"Closing price\")\ndf = data\nprint(df)\n\ndata.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_plot = ['Open', 'High', 'Low','Close']\naxes = data[cols_plot].plot(marker='.', alpha=0.5, linestyle='None', figsize=(11, 9), subplots=True)\nfor ax in axes:\n    ax.set_ylabel('Daily trade')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(data['Close'], label=\"Close price\")\nplt.xlabel(\"Timestamp\")\nplt.ylabel(\"Closing price\")\ndf = data\nprint(df)\n\ndf.describe().transpose()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = []\nY = []\nwindow_size=100\nfor i in range(1 , len(df) - window_size -1 , 1):\n    first = df.iloc[i,2]\n    temp = []\n    temp2 = []\n    for j in range(window_size):\n        temp.append((df.iloc[i + j, 2] - first) / first)\n    temp2.append((df.iloc[i + window_size, 2] - first) / first)\n    X.append(np.array(temp).reshape(100, 1))\n    Y.append(np.array(temp2).reshape(1, 1))\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=True)\n\ntrain_X = np.array(x_train)\ntest_X = np.array(x_test)\ntrain_Y = np.array(y_train)\ntest_Y = np.array(y_test)\n\ntrain_X = train_X.reshape(train_X.shape[0],1,100,1)\ntest_X = test_X.reshape(test_X.shape[0],1,100,1)\n\nprint(len(train_X))\nprint(len(test_X))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For creating model and training\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, Bidirectional, TimeDistributed\nfrom tensorflow.keras.layers import MaxPooling1D, Flatten\nfrom tensorflow.keras.regularizers import L1, L2\nfrom tensorflow.keras.metrics import Accuracy\nfrom tensorflow.keras.metrics import RootMeanSquaredError\n\nmodel = tf.keras.Sequential()\n\n# Creating the Neural Network model here...\n# CNN layers\nmodel.add(TimeDistributed(Conv1D(64, kernel_size=3, activation='relu', input_shape=(None, 100, 1))))\nmodel.add(TimeDistributed(MaxPooling1D(2)))\nmodel.add(TimeDistributed(Conv1D(128, kernel_size=3, activation='relu')))\nmodel.add(TimeDistributed(MaxPooling1D(2)))\nmodel.add(TimeDistributed(Conv1D(64, kernel_size=3, activation='relu')))\nmodel.add(TimeDistributed(MaxPooling1D(2)))\nmodel.add(TimeDistributed(Flatten()))\n# model.add(Dense(5, kernel_regularizer=L2(0.01)))\n\n# LSTM layers\nmodel.add(Bidirectional(LSTM(100, return_sequences=True)))\nmodel.add(Dropout(0.25))\nmodel.add(Bidirectional(LSTM(100, return_sequences=False)))\nmodel.add(Dropout(0.5))\n\n#Final layers\nmodel.add(Dense(1, activation='linear'))\nmodel.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])\n\nhistory = model.fit(train_X, train_Y, validation_data=(test_X,test_Y), epochs=40,batch_size=20, verbose=1, shuffle =True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test_X, test_Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted  = model.predict(test_X)\ntest_label = test_Y.reshape(-1,1)\npredicted = np.array(predicted[:,0]).reshape(-1,1)\nlen_t = len(train_X)\nfor j in range(len_t , len_t + len(test_X)):\n    temp = data.iloc[j,3]\n    test_label[j - len_t] = test_label[j - len_t] * temp + temp\n    predicted[j - len_t] = predicted[j - len_t] * temp + temp\nplt.plot(test_label, color = 'red', label = 'Real Stock Price')\nplt.plot(predicted, color = 'green', label = 'Predicted  Stock Price')\nplt.title(' Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel(' Stock Price')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'], label='train loss')\nplt.plot(history.history['val_loss'], label='val loss')\nplt.xlabel(\"epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['mse'], label='train mse')\nplt.plot(history.history['val_mse'], label='val mse')\nplt.xlabel(\"epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['mae'], label='train mae')\nplt.plot(history.history['val_mae'], label='val mae')\nplt.xlabel(\"epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\nprint(model.summary())\nplot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import explained_variance_score, mean_poisson_deviance, mean_gamma_deviance\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import max_error\n\n# predict probabilities for test set\nyhat_probs = model.predict(test_X, verbose=0)\n# reduce to 1d array\nyhat_probs = yhat_probs[:, 0]\n\nvar = explained_variance_score(test_Y.reshape(-1,1), yhat_probs)\nprint('Variance: %f' % var)\n\nr2 = r2_score(test_Y.reshape(-1,1), yhat_probs)\nprint('R2 Score: %f' % var)\n\nvar2 = max_error(test_Y.reshape(-1,1), yhat_probs)\nprint('Max Error: %f' % var2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First we need to save a model\nmodel.save(\"model.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load model\nnew_model = tf.keras.models.load_model(\"./model.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For data preprocessing and analysis part\n#data2 = pd.read_csv('../input/price-volume-data-for-all-us-stocks-etfs/Stocks/aaoi.us.txt')\n#data2 = pd.read_csv('../input/nifty50-stock-market-data/SBIN.csv')\n#data2 = pd.read_csv('../input/stock-market-data/stock_market_data/nasdaq/csv/ACTG.csv')\ndata2 = pd.read_csv(\"../input/amazon-stock-price-all-time/Amazon.csv\")\n# Any CSV or TXT file can be added here....\ndata2.dropna(inplace=True)\ndata2.head()\n\ndata2.reset_index(drop=True, inplace=True)\ndata2.fillna(data.mean(), inplace=True)\ndata2.head()\ndf2 = data2.drop('Date', axis=1)\n\nprint(df2)\n\nX = []\nY = []\nwindow_size=100\nfor i in range(1 , len(df2) - window_size -1 , 1):\n    first = df2.iloc[i,4]\n    temp = []\n    temp2 = []\n    for j in range(window_size):\n        temp.append((df2.iloc[i + j, 4] - first) / first)\n    # for j in range(week):\n    temp2.append((df2.iloc[i + window_size, 4] - first) / first)\n    # X.append(np.array(stock.iloc[i:i+window_size,4]).reshape(50,1))\n    # Y.append(np.array(stock.iloc[i+window_size,4]).reshape(1,1))\n    # print(stock2.iloc[i:i+window_size,4])\n    X.append(np.array(temp).reshape(100, 1))\n    Y.append(np.array(temp2).reshape(1, 1))\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=True)\n\ntrain_X = np.array(x_train)\ntest_X = np.array(x_test)\ntrain_Y = np.array(y_train)\ntest_Y = np.array(y_test)\n\ntrain_X = train_X.reshape(train_X.shape[0],1,100,1)\ntest_X = test_X.reshape(test_X.shape[0],1,100,1)\n\nprint(len(train_X))\nprint(len(test_X))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataX = pd.read_csv(\"../input/amazon-stock-price-all-time/Amazon.csv\")\ndataY = pd.read_csv(\"../input/amazon-stock-price-all-time/Amazon.csv\")\ndataX.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_date = '2020-01-01'\nend_date = '2021-11-29'\n\nstart = '2018-01-01'\nend = '2020-01-01'\n\nfill = (dataX['Date']>=start_date) & (dataX['Date']<=end_date)\ndataX = dataX.loc[fill]\ndataX","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fill2 = (dataY['Date']>=start) & (dataY['Date']<=end)\ndataY = dataY.loc[fill2]\ndataY","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom sklearn.metrics import r2_score,mean_squared_error\n\nsns_plot = sns.distplot(dataX['Close'])\nsns_plot2 = sns.distplot(dataY['Close'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(4, 2, figsize = (15, 13))\nsns.boxplot(x= dataX[\"Close\"], ax = ax[0,0])\nsns.distplot(dataX['Close'], ax = ax[0,1])\nsns.boxplot(x= dataX[\"Open\"], ax = ax[1,0])\nsns.distplot(dataX['Open'], ax = ax[1,1])\nsns.boxplot(x= dataX[\"High\"], ax = ax[2,0])\nsns.distplot(dataX['High'], ax = ax[2,1])\nsns.boxplot(x= dataX[\"Low\"], ax = ax[3,0])\nsns.distplot(dataX['Low'], ax = ax[3,1])\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(4, 2, figsize = (15, 13))\nsns.boxplot(x= dataY[\"Close\"], ax = ax[0,0])\nsns.distplot(dataY['Close'], ax = ax[0,1])\nsns.boxplot(x= dataY[\"Open\"], ax = ax[1,0])\nsns.distplot(dataY['Open'], ax = ax[1,1])\nsns.boxplot(x= dataY[\"High\"], ax = ax[2,0])\nsns.distplot(dataY['High'], ax = ax[2,1])\nsns.boxplot(x= dataY[\"Low\"], ax = ax[3,0])\nsns.distplot(dataY['Low'], ax = ax[3,1])\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.heatmap(dataX.corr(),cmap=plt.cm.Reds,annot=True)\nplt.title('Heatmap displaying the relationship between the features of the data (During COVID)',\n         fontsize=13)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.heatmap(dataY.corr(),cmap=plt.cm.Blues,annot=True)\nplt.title('Heatmap displaying the relationship between the features of the data (Before COVID)',\n         fontsize=13)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}